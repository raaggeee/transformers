# Implementing Transformers from Scratch
## Diagram 
<img width="683" height="896" alt="image" src="https://github.com/user-attachments/assets/cb3a7b5c-e9a2-4ef1-a5ea-cfd2f5ba4960" />

## Layers
## Encoder
### For Input
- [ ] Embeddings
- [ ] Positional Encoding
### For Encoder Layer
- [x] Multihead Attention
- [ ] Feed Forward Neural Network
- [ ] Add and Normalize

## Decoder
### For Input
- [ ] Embeddings
- [ ] Positional Encoding
### For Decoder Layer
- [x] Masked Multi Head Attention
- [ ] Cross Encoding
- [ ] Feed Forward Nerual Network
- [ ] Add and Normalize

## Reference https://medium.com/data-science/a-complete-guide-to-write-your-own-transformers-29e23f371ddd
